{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/luis/shrec_modules/cvae_shrec\")\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from modules.shrec import utils,cvae_parameters, cvae, experiment, experiment_parameters, shrec2019utils, generator_images_rotate, cvae_vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the parameters for training\n",
    "Notebook for training the Conditional Variational Autoencoder with a VGG network [Kalliatakis G.](https://github.com/GKalliatakis/Keras-VGG16-places365) pretrained on the [Places](http://places2.csail.mit.edu/) dataset as the encoding neural network. \n",
    "The parameters for training. The number of encoding layers gets overriden by the VGG encoding network parameters. The decoding neural network is an inverse VGG-like network as in the train_simple_vae notebook. Other parameters include:\n",
    "\n",
    "- **epochs**: Number of epochs for training\n",
    "- **batch_size**: Number of images per batch of training\n",
    "- **type_layer**: Type of layers used in the encoding and decoding networ. Either \"convolutional\" or \"dense\"\n",
    "- **num_encoding_layers**: Number of the max pooling layers in the encoding network\n",
    "- **num_decoding_layers**: Number of the upsampling layers in the decoding network\n",
    "- **var_x**: Variance of the normal distribution for p(x|z,y)\n",
    "- **r_loss**: Type of loss chosen for reconstruction (either binary cross entropy \"binary\" or mean squared error \"mse\")\n",
    "- **latent_dim**: Dimensions of the latent space of the conditional variational autoencoder\n",
    "- **intermediate_dim** Defines the number of filters in the case of the convolutions or the number of neurons in a convolutional or dense layer respectively.\n",
    "- **learning_rate**: Learning rate of the optimizer used\n",
    "- **image_shape**: Shape of the images used for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 32\n",
    "type_layer = \"convolutional\"\n",
    "num_encoding_layers = 5\n",
    "num_decoding_layers = 5\n",
    "intermediate_dim  =  64\n",
    "num_classes = 10\n",
    "latent_dim = 100\n",
    "learning_rate = 0.001\n",
    "image_shape = (256,256,3)\n",
    "var_x = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data generator\n",
    "The training of the network is done via a generator. The generator is created from a list of the paths to the corresponding images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial index barn 0 70\n",
      "Initial index beach 0 70\n",
      "Initial index bedroom 0 70\n",
      "Initial index castle 0 70\n",
      "Initial index classroom 0 70\n",
      "Initial index desert 0 70\n",
      "Initial index kitchen 0 70\n",
      "Initial index library 0 70\n",
      "Initial index mountain 0 70\n",
      "Initial index river 0 70\n",
      "Initial index barn 0 700\n",
      "Initial index beach 0 700\n",
      "Initial index bedroom 0 700\n",
      "Initial index castle 0 700\n",
      "Initial index classroom 0 700\n",
      "Initial index desert 0 700\n",
      "Initial index kitchen 0 700\n",
      "Initial index library 0 700\n",
      "Initial index mountain 0 700\n",
      "Initial index river 0 700\n",
      "Categories of renders ['barn' 'beach' 'bedroom' 'castle' 'classroom' 'desert' 'kitchen'\n",
      " 'library' 'mountain' 'river'] amount 10\n",
      "Categories of images ['barn' 'beach' 'bedroom' 'castle' 'classroom' 'desert' 'kitchen'\n",
      " 'library' 'mountain' 'river'] amount 10\n",
      "Amount of renders 700\n",
      "Amount of images 7000\n"
     ]
    }
   ],
   "source": [
    "main_directory = os.path.dirname(os.getcwd())\n",
    "dataset_directory = os.path.join(main_directory, \"dataset\")\n",
    "path_class_files = os.path.join(dataset_directory, \"class_files\")\n",
    "# Read the file with the names of the models and images that will be used for training\n",
    "cla_file_path_renders = os.path.join(path_class_files, \"SceneIBR2018_Model_Training.cla\")\n",
    "cla_file_path_photographs = os.path.join(path_class_files, \"SceneIBR2018_Image_Training.cla\")\n",
    "\n",
    "# Paths to the corresponding images folder\n",
    "renders_path = os.path.join(dataset_directory,\"renders_256\")\n",
    "photos_path = os.path.join(dataset_directory,\"images_256\")\n",
    "\n",
    "# Renders\n",
    "categories, model_lists = shrec2019utils.obtain_models_classes(cla_file_path_renders)\n",
    "categories_array, models_array = shrec2019utils.categories_model_list_to_array(categories, model_lists)\n",
    "# Photographs\n",
    "categories_photographs, model_lists_photographs = shrec2019utils.obtain_models_classes2018(cla_file_path_photographs)\n",
    "categories_array_photographs, models_array_photographs = shrec2019utils.categories_model_list_to_array(categories_photographs, model_lists_photographs)\n",
    "\n",
    "print(\"Categories of renders {} amount {}\".format(np.unique(categories), len(np.unique(categories))))\n",
    "print(\"Categories of images {} amount {}\".format(np.unique(categories_photographs), len(np.unique(categories_photographs))))\n",
    "print(\"Amount of renders {}\".format(len(models_array)))\n",
    "print(\"Amount of images {}\".format(len(models_array_photographs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join identifiers for rendered images and photographs\n",
    "\n",
    "Create a list comprising both, the identifiers for the renders and the images used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of renders  9100\n",
      "Amount of photographs  7000\n"
     ]
    }
   ],
   "source": [
    "# Join the paths for the training renders and photographs\n",
    "images_paths = []\n",
    "joint_categories = []\n",
    "type_data = []\n",
    "for num_model, model in enumerate(models_array):\n",
    "    for image_path in glob.glob(os.path.join(renders_path,  str(model)+'_*.png')):\n",
    "        images_paths.append(image_path)\n",
    "        joint_categories.append(categories_array[num_model])\n",
    "        type_data.append(\"render\")\n",
    "number_of_renders = len(images_paths)\n",
    "print(\"Amount of renders \",number_of_renders)   \n",
    "for num_model, model in enumerate(models_array_photographs):\n",
    "    for image_path in glob.glob(os.path.join(photos_path,  str(model)+'.png')):\n",
    "        images_paths.append(image_path)\n",
    "        joint_categories.append(categories_array_photographs[num_model])\n",
    "        type_data.append(\"photograph\")\n",
    "number_of_photographs = len(images_paths)-number_of_renders\n",
    "print(\"Amount of photographs \",number_of_photographs)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint categories length 16100\n",
      "Joint paths length 16100\n"
     ]
    }
   ],
   "source": [
    "# Join the categories for each of the paths\n",
    "joint_categories = np.array(joint_categories)\n",
    "joint_categories = shrec2019utils.categories_array_to_numerical(joint_categories)\n",
    "# Transform the categories to one hot encodings\n",
    "joint_categories = utils.one_hotify(joint_categories)\n",
    "print(\"Joint categories length {}\".format(len(joint_categories)))\n",
    "print(\"Joint paths length {}\".format(len(images_paths)))\n",
    "\n",
    "# Create the generator\n",
    "\n",
    "dim = (image_shape[0], image_shape[1]) # resolution of the images\n",
    "n_channels = image_shape[2] # number of color channels\n",
    "generator = generator_images_rotate.ShrecGenerator(images_paths, joint_categories,type_data, dim= dim, n_channels = n_channels, batch_size=batch_size, n_classes = num_classes, flatten=False, resize=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of Conditional Variational Autoencoder\n",
    "In this section an instance of the conditional variational autoencoder class is defined with respect to the parameters previously defined. The class containes the encoding and decoding neural network of the conditional variational autoencoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1028 16:36:42.434905 139956805080896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss is mean squared error\n",
      "Reconstruction loss is mean squared error\n"
     ]
    }
   ],
   "source": [
    "# DEFINE THE NETWORK OF THE CONDITIONAL VARIATIONAL AUTOENCODER\n",
    "cvae_parameters_dict = {\"image_shape\": image_shape,# shape of the input images\n",
    "                       \"num_classes\": num_classes, # number of classes for the scenes\n",
    "                       \"type_layers\":type_layer,# either \"convolutional neural network\" or \"dense neural network\"\n",
    "                       \"num_encoding_layers\": num_encoding_layers, # number of the max pooling layers in the encoding\n",
    "                       \"num_decoding_layers\": num_decoding_layers, # number of the upsampling layers in the decoding\n",
    "                       \"var_x\":var_x, # variance of the normal distribution for p(x|z,y)\n",
    "                       \"r_loss\":\"mse\", # type of loss chosen (either binary cross entropy \"binary\" or mean squared error \"mse\")\n",
    "                       \"latent_dim\":latent_dim, # dimensions of the latent space of the conditional variational autoencoder\n",
    "                       \"intermediate_dim\":intermediate_dim, # defines the number of filters in the convolutions\n",
    "                       \"learning_rate\":learning_rate # learning rate of the optimizer\n",
    "                       }\n",
    "# Define the parameters for the CVAE\n",
    "parameters = cvae_parameters.CVAEParams(**cvae_parameters_dict)\n",
    "# Define the CVAE\n",
    "cond_vae = cvae_vgg.CVAE_VGG(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training experiment\n",
    "Define an experiment for training. The experiment takes the instance of the conditional variational autoencoder, the experiment parameters and a folder for saving the tensorboard log files and the trained model weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1028 16:36:49.757392 139956805080896 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1028 16:36:52.139703 139956805080896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W1028 16:36:52.147371 139956805080896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W1028 16:36:52.148162 139956805080896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 422s - loss: 1500.2850 - r_loss: 1487.0688 - kl_loss: 10.6430 - mean_squared_error: 0.0151 - classification_loss: 2.3392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1028 16:43:55.099596 139956805080896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500\n",
      " - 427s - loss: 1459.2462 - r_loss: 1456.6420 - kl_loss: 0.0658 - mean_squared_error: 0.0148 - classification_loss: 2.3024\n",
      "Epoch 3/500\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters for training\n",
    "experiment_params_dict = {\"epochs\": epochs,\n",
    "                         \"batch_size\": batch_size}\n",
    "exp_parameters  = experiment_parameters.ExperimentParams(**experiment_params_dict)\n",
    "\n",
    "# Define an experiment\n",
    "saving_model_folder = os.path.join(main_directory, \"trained_models\",\"vgg_submission\")\n",
    "\n",
    "\n",
    "exp = experiment.Experiment(cond_vae, exp_parameters, None, saving_model_folder)\n",
    "\n",
    "exp.run_image_generator(generator) # Training of the CVAE with the generator as a source of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
